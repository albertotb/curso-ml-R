---
title: "Tuneando Random Forest"
author: Victor Gallego y Roi Naveiro
date: "22/04/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(dplyr)
library(randomForest)
library(ggplot2)
library(reshape2)
```

En este ejercicio, entrenaremos random forest utilizando los datos que nos habéis proporcionado.

## Lectura y Limpieza de los datos

Carga el Dataset del ENSE

```{r, message=F}
## Código
```

Crea la variable target (True si edCNO_AS es igual a rawCNO_AS, y False en otro caso).
Además, elimina las variables categóricas con más de 53 categorías, pues el paquete randomForest no las sabe manejar. Además, convierte a factor el resto de variables categóricas.

```{r, message=F}
## Código
```

## Train y Test

Crea train y test con proporciones 80/20.

```{r, message=F}
## Código
```


## Entrenamiento 

Entrena un random forest validando el parámetro mtry. Usa la función tuneRF. Pinta la gráfica de metry frente al OOB error.
```{r}
## Código
```


## Análisis test set.

Predice sobre el test y dibuja la curva precisión recall.

```{r, message=F}
## Código
precision = function(df, depth){
  tot = dim(df)[1]
  inspect = floor(depth*tot)
  return(sum(df[1:inspect,2] == FALSE)/inspect)
}

recall = function(df, depth){
  tot = dim(df)[1]
  totF = sum(df[,2] == FALSE)
  inspect = floor(depth*tot)
  return(sum(df[1:inspect,2] == FALSE)/totF)
}


precRecallPlot = function(df, min = 0.0001, max = 0.5, step = 0.001){
  Recall = c()
  Precision = c()
  Depth = c()
  for(i in seq(min, max, step)){
    Depth = c(Depth, i)
    Recall = c(Recall, recall(df, i))
    Precision = c(Precision, precision(df, i))
  }
  
  results = data.frame(Depth = Depth, recall = Recall, precision = Precision)
  meltedResults = melt(results, id = "Depth")
  p = ggplot(meltedResults, aes(x = Depth, y = value, color = variable))
  p = p + geom_line() + xlab("Depth") + ylab("Value")
  p
}
```
